{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c409679b",
   "metadata": {},
   "source": [
    "## 4-dim grid example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1720590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load packages\n",
    "from NashQLearn import Player, Grid, NashQLearning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c116587f",
   "metadata": {},
   "source": [
    "This notebook applies the Nash Q Learning algorithm to the following multiagent problem : two robots placed on a grid need to reach the reward. Robots are allowed to move up, down, to the left, and to the right, or to stay at their current position. Robots are not allowed to be on the same tile unless it is the reward tile.\n",
    "\n",
    "### Prepare the game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6a6e69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the two players\n",
    "player1 = Player([3,0])\n",
    "player2 = Player([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b91d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the grid\n",
    "grid = Grid(length = 4,\n",
    "            width = 4,\n",
    "            players = [player1,player2],\n",
    "           obstacle_coordinates = [[0,0], [1,0],[1,2],[1,3],[0,3],[2,3],[3,1]],\n",
    "           reward_coordinates = [0,2],\n",
    "           reward_value = 20,\n",
    "           collision_penalty = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abdc9942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available joint states : 73\n",
      "[[[0, 1], [0, 2]], [[0, 1], [1, 1]], [[0, 1], [2, 0]], [[0, 1], [2, 1]], [[0, 1], [2, 2]], [[0, 1], [3, 0]], [[0, 1], [3, 2]], [[0, 1], [3, 3]], [[0, 2], [0, 1]], [[0, 2], [1, 1]], [[0, 2], [2, 0]], [[0, 2], [2, 1]], [[0, 2], [2, 2]], [[0, 2], [3, 0]], [[0, 2], [3, 2]], [[0, 2], [3, 3]], [[1, 1], [0, 1]], [[1, 1], [0, 2]], [[1, 1], [2, 0]], [[1, 1], [2, 1]], [[1, 1], [2, 2]], [[1, 1], [3, 0]], [[1, 1], [3, 2]], [[1, 1], [3, 3]], [[2, 0], [0, 1]], [[2, 0], [0, 2]], [[2, 0], [1, 1]], [[2, 0], [2, 1]], [[2, 0], [2, 2]], [[2, 0], [3, 0]], [[2, 0], [3, 2]], [[2, 0], [3, 3]], [[2, 1], [0, 1]], [[2, 1], [0, 2]], [[2, 1], [1, 1]], [[2, 1], [2, 0]], [[2, 1], [2, 2]], [[2, 1], [3, 0]], [[2, 1], [3, 2]], [[2, 1], [3, 3]], [[2, 2], [0, 1]], [[2, 2], [0, 2]], [[2, 2], [1, 1]], [[2, 2], [2, 0]], [[2, 2], [2, 1]], [[2, 2], [3, 0]], [[2, 2], [3, 2]], [[2, 2], [3, 3]], [[3, 0], [0, 1]], [[3, 0], [0, 2]], [[3, 0], [1, 1]], [[3, 0], [2, 0]], [[3, 0], [2, 1]], [[3, 0], [2, 2]], [[3, 0], [3, 2]], [[3, 0], [3, 3]], [[3, 2], [0, 1]], [[3, 2], [0, 2]], [[3, 2], [1, 1]], [[3, 2], [2, 0]], [[3, 2], [2, 1]], [[3, 2], [2, 2]], [[3, 2], [3, 0]], [[3, 2], [3, 3]], [[3, 3], [0, 1]], [[3, 3], [0, 2]], [[3, 3], [1, 1]], [[3, 3], [2, 0]], [[3, 3], [2, 1]], [[3, 3], [2, 2]], [[3, 3], [3, 0]], [[3, 3], [3, 2]], [[0, 2], [0, 2]]]\n"
     ]
    }
   ],
   "source": [
    "joint_states = grid.joint_states()\n",
    "print('Available joint states : %s'%len(joint_states))#Correct\n",
    "print(joint_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10282bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['left', [0, 1]],\n",
       " ['down', [0, 1]],\n",
       " ['left', [0, 2]],\n",
       " ['right', [0, 2]],\n",
       " ['up', [0, 2]],\n",
       " ['up', [1, 1]],\n",
       " ['down', [1, 1]],\n",
       " ['left', [2, 0]],\n",
       " ['down', [2, 0]],\n",
       " ['right', [2, 1]],\n",
       " ['left', [2, 2]],\n",
       " ['up', [2, 2]],\n",
       " ['right', [3, 0]],\n",
       " ['up', [3, 0]],\n",
       " ['down', [3, 0]],\n",
       " ['right', [3, 2]],\n",
       " ['down', [3, 2]],\n",
       " ['left', [3, 3]],\n",
       " ['right', [3, 3]],\n",
       " ['up', [3, 3]]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walls = grid.identify_walls()\n",
    "walls  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c924a07",
   "metadata": {},
   "source": [
    "###  Run the Nash Q Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28406b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nashQ = NashQLearning(grid, \n",
    "                      max_iter = 2000,\n",
    "                      discount_factor = 0.9,\n",
    "                      learning_rate = 0.7,\n",
    "                      epsilon = 0.4,\n",
    "                      decision_strategy = 'epsilon-greedy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7c49ebe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 73/73 [00:00<00:00, 2437.05it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [09:04<00:00,  3.68it/s]\n"
     ]
    }
   ],
   "source": [
    "#Retrieve the updated Q matrix after fitting the algorithm\n",
    "Q0, Q1 = nashQ.fit(return_history = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa2f5917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 0], [2, 2]]\n",
      "[[2, 0], [2, 1]]\n",
      "[[3, 0], [1, 1]]\n",
      "[[2, 0], [0, 1]]\n",
      "[[2, 1], [0, 2]]\n",
      "[[1, 1], [0, 2]]\n",
      "[[0, 1], [0, 2]]\n",
      "[[0, 2], [0, 2]]\n"
     ]
    }
   ],
   "source": [
    "#Best path followed by each player given the values in the q tables\n",
    "p0, p1 = nashQ.get_best_policy(Q0,Q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85a96bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 0 follows the  policy : left-right-left-up-left-left-up of length 7\n",
      "Player 1 follows the  policy : down-left-left-up-stay-stay-stay of length 7\n"
     ]
    }
   ],
   "source": [
    "print('Player 0 follows the  policy : %s of length %s' %('-'.join(p0),len(p0)))\n",
    "print('Player 1 follows the  policy : %s of length %s'%('-'.join(p1),len(p1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
